{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is regularization in the context of deep learning? Why is it important.\n",
        "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff\n",
        "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the modelG\n",
        "4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models.\n",
        "\n",
        "Regularization in the context of deep learning is a technique used to prevent overfitting by adding a penalty to the loss function. Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise, leading to poor performance on new, unseen data. Regularization helps to generalize the model by discouraging overly complex models.\n",
        "\n",
        "### Importance of Regularization\n",
        "\n",
        "1. **Prevents Overfitting**: Regularization helps prevent the model from becoming too complex and capturing noise in the training data.\n",
        "2. **Improves Generalization**: By penalizing large weights, regularization encourages the model to find simpler patterns that are more likely to generalize well to new data.\n",
        "3. **Stabilizes Training**: It can help in stabilizing the training process by avoiding large updates to the model weights.\n",
        "\n",
        "### Bias-Variance Tradeoff and Regularization\n",
        "\n",
        "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between two sources of error in a model:\n",
        "\n",
        "- **Bias**: Error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to underfit the data, missing important patterns.\n",
        "- **Variance**: Error due to too much complexity in the learning algorithm. High variance can cause the model to overfit the training data, capturing noise as if it were a true pattern.\n",
        "\n",
        "Regularization helps in addressing this tradeoff by adding a penalty to the loss function, which discourages the model from fitting the noise in the training data (high variance) and helps it to generalize better (low variance).\n",
        "\n",
        "### L1 and L2 Regularization\n",
        "\n",
        "- **L1 Regularization (Lasso)**:\n",
        "  - **Penalty Calculation**: The penalty term is the sum of the absolute values of the weights, \\(\\lambda \\sum |w_i|\\).\n",
        "  - **Effect on the Model**: L1 regularization can lead to sparse models where some weights are exactly zero, effectively performing feature selection.\n",
        "\n",
        "- **L2 Regularization (Ridge)**:\n",
        "  - **Penalty Calculation**: The penalty term is the sum of the squared values of the weights, \\(\\lambda \\sum w_i^2\\).\n",
        "  - **Effect on the Model**: L2 regularization tends to shrink the weights but does not make them exactly zero. It distributes the penalty more evenly and is often preferred when all input features are expected to be useful.\n",
        "\n",
        "### Role of Regularization in Preventing Overfitting and Improving Generalization\n",
        "\n",
        "Regularization techniques, such as L1 and L2, add a penalty for large weights to the loss function. This discourages the model from becoming overly complex and helps it to generalize better to new, unseen data. Here's how regularization helps:\n",
        "\n",
        "1. **Controls Model Complexity**: By penalizing large weights, regularization prevents the model from becoming too complex and overfitting the training data.\n",
        "2. **Encourages Simpler Models**: Simpler models are less likely to capture noise in the training data, leading to better performance on test data.\n",
        "3. **Stabilizes Weight Updates**: Regularization can prevent large updates to the weights, leading to a more stable and consistent training process.\n",
        "4. **Feature Selection (L1)**: L1 regularization can effectively perform feature selection by shrinking some weights to zero, thereby removing less important features from the model.\n",
        "5. **Improves Generalization**: By reducing overfitting, regularization improves the model's ability to generalize to new, unseen data.\n",
        "\n",
        "In summary, regularization is a crucial technique in deep learning that helps in managing the bias-variance tradeoff, preventing overfitting, and improving the generalization of models."
      ],
      "metadata": {
        "id": "Dq-j2x10TaJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference.\n",
        "2.  Describe the concept of Early ztopping as a form of regularization. How does it help prevent overfitting during the training process.\n",
        "3. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting.\n",
        "\n",
        "### Dropout Regularization\n",
        "\n",
        "**Dropout** is a regularization technique used in deep learning to prevent overfitting. It involves randomly \"dropping out\" a fraction of the neurons during training at each iteration, meaning these neurons are temporarily removed from the network. This forces the network to not rely on specific neurons and encourages it to learn more robust features.\n",
        "\n",
        "**How Dropout Works:**\n",
        "- During each training iteration, a certain percentage (dropout rate) of neurons are randomly selected to be ignored or \"dropped out.\"\n",
        "- The forward and backward passes are performed only on the remaining neurons.\n",
        "- During inference (testing), no neurons are dropped. Instead, the weights are scaled down by the dropout rate to balance the impact of the previously dropped neurons.\n",
        "\n",
        "**Impact of Dropout on Model Training and Inference:**\n",
        "- **Training Phase**: Dropout forces the network to be more resilient by not relying on specific neurons, leading to the development of redundant representations that improve robustness.\n",
        "- **Inference Phase**: The network uses all neurons but with scaled-down weights to maintain the same expected output.\n",
        "\n",
        "### Early Stopping as a Form of Regularization\n",
        "\n",
        "**Early Stopping** is a regularization technique used to prevent overfitting by stopping the training process before the model begins to overfit the training data.\n",
        "\n",
        "**How Early Stopping Works:**\n",
        "- Monitor the performance of the model on a validation dataset during training.\n",
        "- If the performance on the validation dataset starts to degrade while the performance on the training dataset continues to improve, it indicates overfitting.\n",
        "- Training is stopped when the validation performance stops improving for a specified number of epochs (patience).\n",
        "\n",
        "**Impact of Early Stopping:**\n",
        "- Prevents overfitting by halting the training process before the model starts to memorize the training data.\n",
        "- Saves computational resources by avoiding unnecessary training epochs.\n",
        "- Helps in obtaining a model that generalizes better to unseen data.\n",
        "\n",
        "### Batch Normalization as a Form of Regularization\n",
        "\n",
        "**Batch Normalization** is a technique that normalizes the inputs of each layer to have a mean of zero and a variance of one, effectively stabilizing and speeding up the training process.\n",
        "\n",
        "**How Batch Normalization Works:**\n",
        "- During training, for each mini-batch, the mean and variance of the activations are calculated.\n",
        "- The activations are then normalized using these batch statistics.\n",
        "- Two learnable parameters, scale (gamma) and shift (beta), are introduced to allow the network to undo the normalization if necessary.\n",
        "\n",
        "**Impact of Batch Normalization:**\n",
        "- **Regularization**: Batch Normalization introduces noise in the estimates of mean and variance due to the small batch sizes, which acts as a form of regularization. This noise makes the network more robust and less likely to overfit.\n",
        "- **Training Efficiency**: It allows for higher learning rates and reduces the sensitivity to initialization, speeding up the convergence.\n",
        "- **Stabilization**: By normalizing the activations, it mitigates issues related to internal covariate shift, leading to a more stable training process.\n",
        "\n",
        "### Role in Preventing Overfitting\n",
        "\n",
        "1. **Dropout**: Encourages the network to learn redundant representations by randomly dropping neurons, thus making the model more robust and less prone to overfitting.\n",
        "2. **Early Stopping**: Stops the training process before the model starts overfitting, ensuring that the model retains good generalization capabilities.\n",
        "3. **Batch Normalization**: Regularizes the model by introducing noise in the mini-batch statistics and stabilizes training, helping the model generalize better.\n",
        "\n",
        "In summary, Dropout, Early Stopping, and Batch Normalization are all effective regularization techniques that help prevent overfitting and improve the generalization of deep learning models."
      ],
      "metadata": {
        "id": "Lzv0bYEaTje9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77KjizSxTnNI",
        "outputId": "83e9506a-36e6-4462-a628-7666a6ca4e02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "eMqVjnTnTnJz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist"
      ],
      "metadata": {
        "id": "CkpspKy7TnGr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY5EL1s7TnD8",
        "outputId": "38788d92-ac2b-4238-b34b-2bc4005ed056"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
        "X_test = tf.keras.utils.normalize(X_test, axis=1)"
      ],
      "metadata": {
        "id": "-uVBkcEZTnBb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "earlyStopping = EarlyStopping(monitor='val_loss',patience=3)"
      ],
      "metadata": {
        "id": "StLPnfGvWNpA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name=\"inputLayer\"),\n",
        "          tf.keras.layers.Dense(300, activation=\"relu\", name=\"hiddenLayer1\", kernel_regularizer='l2'),\n",
        "          tf.keras.layers.BatchNormalization(),  # Batch Normalization Layer\n",
        "          tf.keras.layers.Dense(100, activation=\"relu\", name=\"hiddenLayer2\", kernel_regularizer='l2'),\n",
        "          tf.keras.layers.BatchNormalization(),  # Batch Normalization Layer\n",
        "          tf.keras.layers.Dense(10, activation=\"softmax\", name=\"outputLayer\")]\n",
        "\n",
        "model = tf.keras.models.Sequential(LAYERS)"
      ],
      "metadata": {
        "id": "LLZJwqaUVANw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "eyCDY78VUb0y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_SET = (X_test, y_test)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=VALIDATION_SET, batch_size=32, callbacks=[earlyStopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVfocH79WL3C",
        "outputId": "10b4178f-105f-4e97-cc6c-79064c2079e4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 4.4401 - accuracy: 0.9022 - val_loss: 2.9247 - val_accuracy: 0.9543\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 2.1131 - accuracy: 0.9548 - val_loss: 1.4492 - val_accuracy: 0.9632\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 1.0648 - accuracy: 0.9663 - val_loss: 0.7790 - val_accuracy: 0.9635\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.5874 - accuracy: 0.9689 - val_loss: 0.4591 - val_accuracy: 0.9678\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.3749 - accuracy: 0.9692 - val_loss: 0.3207 - val_accuracy: 0.9661\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2720 - accuracy: 0.9713 - val_loss: 0.2541 - val_accuracy: 0.9674\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2268 - accuracy: 0.9710 - val_loss: 0.2239 - val_accuracy: 0.9712\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2031 - accuracy: 0.9729 - val_loss: 0.2165 - val_accuracy: 0.9679\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1917 - accuracy: 0.9733 - val_loss: 0.2059 - val_accuracy: 0.9716\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1862 - accuracy: 0.9735 - val_loss: 0.2044 - val_accuracy: 0.9674\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1816 - accuracy: 0.9740 - val_loss: 0.1929 - val_accuracy: 0.9711\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1788 - accuracy: 0.9744 - val_loss: 0.2133 - val_accuracy: 0.9634\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1830 - accuracy: 0.9736 - val_loss: 0.1941 - val_accuracy: 0.9712\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1815 - accuracy: 0.9737 - val_loss: 0.2042 - val_accuracy: 0.9674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oapBFzmBWQu4",
        "outputId": "a0d6b144-439d-40da-d44a-52101a98236c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2042 - accuracy: 0.9674\n",
            "loss:  0.20420731604099274\n",
            "Accuracy:  0.9674000144004822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KxOrju7zW3Cq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}